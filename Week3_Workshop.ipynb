{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onni-Q/workshop_notebooks/blob/main/Week3_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI4JIPFdFe1F"
      },
      "source": [
        "Please run the cell below to set up the environment.\n",
        "\n",
        "**To get a complete, please complete 1 of the 2 parts (unsupervised and supvised machine learning) fully and the other at least 50%. The section 'The Workings of Decision Trees' is entirely optional.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_11ogtUxiPzO"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import learntools.core\n",
        "  from learntools.core import binder\n",
        "  binder.bind(globals())\n",
        "except:\n",
        "  !pip install git+https://github.com/Kaggle/learntools.git\n",
        "  import learntools.core\n",
        "  from learntools.core import binder\n",
        "  binder.bind(globals())\n",
        "\n",
        "try:\n",
        "  import palettable\n",
        "except:\n",
        "  !pip install palettable\n",
        "  import palettable\n",
        "\n",
        "\n",
        "if not(os.path.exists('ml_workshop_tests.py')):\n",
        "  print('Downloading ml_workshop_tests.py\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/ml_workshop_tests.py\n",
        "\n",
        "\n",
        "if not(os.path.exists('114_congress.csv')):\n",
        "  print('Downloading 114_congress.csv\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/main/114_congress.csv\n",
        "\n",
        "if not(os.path.exists('spotify-dataset.csv')):\n",
        "  print('Downloading spotify-dataset.csv\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/spotify-dataset.csv\n",
        "\n",
        "if not(os.path.exists('CDAIML.py')):\n",
        "  print('Downloading CDAIML.py\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/CDAIML.py\n",
        "\n",
        "from ml_workshop_tests import *\n",
        "from CDAIML import display_youtube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtnovPsH9zUt"
      },
      "source": [
        "In today's workshop, we focus on real-life social and cultural datasets, which often lack the quality to be processed easily. The kind of social and cultural data we are dealing with is vast and unorganized, which makes organizing it for analysis no easy task. In reality, you will spend most of your time on working through such data challenges.\n",
        "\n",
        "In this workshop, you will practice once more: cleaning real-world data, applying k-means clustering for exploratory analysis in another context and implementing decision trees and random forests for classification. And, you will practice more programming with Gemini.\n",
        "\n",
        "We start by exploring political and social community data using clustering we know from the lecture. It is very effective and easy to use. We will gain some real insights about US politics as well as communities in social-networking sites. Next to this unsupervised-learning task, we will go more into the details of particular supervised learning that is widely used in the second part of the workshop.\n",
        "\n",
        "First load all the standard Pandas, etc. Python libraries, which should be familiar to you.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kpEaZ_rq9zUt"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTPCigp79zUt"
      },
      "source": [
        "## Unsupervised Machine Learning with Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqnIlxJ9zUt"
      },
      "source": [
        "In the first part, we dig deeper into digital methodologies and data exploration using a particular method called clustering, which is closely related to the understanding of political and social communities. We rely on our trusted k-means clustering to explore political voting behaviour.\n",
        "\n",
        "As a reminder, k-means tries to develop clusters by\n",
        "1. initialising a pre-defined number (k) of randomly chosen centroids in the feature space. Centroids are simply the centre points of clusters.\n",
        "1. The algorithm assigns each observation to the cluster with the closest centroid.\n",
        "1. Based on how balanced this assignment is, the centroids are recalculated and steps 1 and 2 are repeated until the algorithm balances out.\n",
        "\n",
        "We will use k-means to understand voting behaviour in the US senate. We chose a time now long ago, when the Senate was less partisan, as we would like to investigate voting behaviour that crosses party lines. The data is a subset of the data from https://www.dataquest.io/blog/k-means-clustering-us-senators/.\n",
        "\n",
        "Please, run the cell below to create the `congress_114` dataframe, which contains the voting behaviour of 114th US Senate. According to Wikipedia (https://en.wikipedia.org/wiki/114th_United_States_Congress), the 114th Congress met in Washington, D.C. from 3 January 2015 to 3 January 2017, during the final two years of Barack Obama's presidency.\n",
        "\n",
        "The 2014 elections gave the Republicans control of the Senate and control of both houses of Congress for the first time since the 109th Congress. With 247 seats in the House of Representatives and 54 seats in the Senate, this Congress began with the largest Republican majority since the 71st Congress of 1929-1931. There are 23 Democrats, 1 Independent and 33 Republicans in our dataset. As said, this does not represent the full 114th congress but a sample.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6xIOxpJG9zUt"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "congress_114 = pd.read_csv(\"114_congress.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqbgG-X-9zUt"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "To warm up, please check the first five entries of the dataset. It contains the name of a particular senator, their party and home state as well as for each bill whether the senator voted for a bill (1) or against it (0). Check out the first 5 rows of `congress_114` with `head()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yG_CB4sK9zUu",
        "outputId": "d38c4de2-2cf0-4f03-fea3-e4e48036d015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index           name    party    state  bill-1  bill-4  bill-5  bill-6  \\\n",
              "0      0     Alexander        R       TN      0.0     1.0     1.0     1.0   \n",
              "1      1        Ayotte        R       NH      0.0     1.0     1.0     1.0   \n",
              "2      2       Baldwin        D       WI      1.0     0.0     0.0     1.0   \n",
              "3      3      Barrasso        R       WY      0.0     1.0     1.0     1.0   \n",
              "4      4        Bennet        D       CO      0.0     0.0     0.0     1.0   \n",
              "\n",
              "   bill-7  bill-8  bill-9  bill-10  bill-20  bill-26  bill-32  bill-38  \\\n",
              "0     1.0     0.0     0.0      1.0      1.0      1.0      0.0      0.0   \n",
              "1     1.0     0.0     0.0      1.0      0.0      1.0      0.0      1.0   \n",
              "2     0.0     1.0     0.0      1.0      0.0      0.0      1.0      1.0   \n",
              "3     1.0     0.0     1.0      1.0      1.0      1.0      0.0      0.0   \n",
              "4     0.0     1.0     0.0      1.0      0.0      0.0      0.0      1.0   \n",
              "\n",
              "   bill-39  bill-44  bill-47  \n",
              "0      0.0      0.0      0.0  \n",
              "1      0.0      1.0      0.0  \n",
              "2      0.0      1.0      1.0  \n",
              "3      1.0      0.0      0.0  \n",
              "4      0.0      1.0      0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d88db96b-a3fb-4502-89dd-3e5f6c5a66a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>name</th>\n",
              "      <th>party</th>\n",
              "      <th>state</th>\n",
              "      <th>bill-1</th>\n",
              "      <th>bill-4</th>\n",
              "      <th>bill-5</th>\n",
              "      <th>bill-6</th>\n",
              "      <th>bill-7</th>\n",
              "      <th>bill-8</th>\n",
              "      <th>bill-9</th>\n",
              "      <th>bill-10</th>\n",
              "      <th>bill-20</th>\n",
              "      <th>bill-26</th>\n",
              "      <th>bill-32</th>\n",
              "      <th>bill-38</th>\n",
              "      <th>bill-39</th>\n",
              "      <th>bill-44</th>\n",
              "      <th>bill-47</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Alexander</td>\n",
              "      <td>R</td>\n",
              "      <td>TN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ayotte</td>\n",
              "      <td>R</td>\n",
              "      <td>NH</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Baldwin</td>\n",
              "      <td>D</td>\n",
              "      <td>WI</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Barrasso</td>\n",
              "      <td>R</td>\n",
              "      <td>WY</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Bennet</td>\n",
              "      <td>D</td>\n",
              "      <td>CO</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d88db96b-a3fb-4502-89dd-3e5f6c5a66a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d88db96b-a3fb-4502-89dd-3e5f6c5a66a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d88db96b-a3fb-4502-89dd-3e5f6c5a66a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-176e46c7-6b2b-4496-b992-34c49a2b297f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-176e46c7-6b2b-4496-b992-34c49a2b297f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-176e46c7-6b2b-4496-b992-34c49a2b297f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "congress_114",
              "summary": "{\n  \"name\": \"congress_114\",\n  \"rows\": 58,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 0,\n        \"max\": 57,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          0,\n          5,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 58,\n        \"samples\": [\n          \"   Alexander \",\n          \"  Blumenthal \",\n          \"       Flake \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"party\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"     R \",\n          \"     D \",\n          \"     I \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"    MS \",\n          \"    PA \",\n          \"    DE \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43409659020091396,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49114974312875154,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.500626174321759,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22528177844479147,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4981167541368987,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49114974312875154,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49811675413689877,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49353590977312783,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-26\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4798574349686965,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-32\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.48666426339228747,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-38\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.502500015586229,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-39\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5037453706946007,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-44\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.500626174321759,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill-47\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4689614184877847,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "congress_114.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell to get a hint and finally the solution. You know these cells by now, and we will not introduce them anymore."
      ],
      "metadata": {
        "id": "YFEI0NuKAY4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q0.hint()\n",
        "q0.solution()"
      ],
      "metadata": {
        "id": "1y3Crt36av7t",
        "outputId": "7fbd52e9-f2f4-4475-932f-9407738f5766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"questionId\": \"0_Exercise0\", \"learnToolsVersion\": \"0.3.5\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hint: Use the .head() method to display the first few rows of a DataFrame. This helps you understand the structure and content of your data. Try: dataframe_name.head(n) where n is the number of rows to display."
            ],
            "text/markdown": "<span style=\"color:#3366cc\">Hint:</span> Use the .head() method to display the first few rows of a DataFrame. This helps you understand the structure and content of your data. Try: dataframe_name.head(n) where n is the number of rows to display."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"questionId\": \"0_Exercise0\", \"learnToolsVersion\": \"0.3.5\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Solution: \n",
              "```python\n",
              "congress_114.head(5)\n",
              "```"
            ],
            "text/markdown": "<span style=\"color:#33cc99\">Solution:</span> \n```python\ncongress_114.head(5)\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9NLotto9zUu"
      },
      "source": [
        "Next check the last five records. You know how of course ...\n",
        "\n",
        "You will see that the last record contains lots of NaN values, which stand in Python for missing values. This is the voting record of a senator who was not able to vote. In real-life datasets, you will see quite a few of these kinds of records - maybe because they never existed or they were not recorded in the first place, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7l_iGAvwv6LB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1.check()\n",
        "#q1.hint()\n",
        "#q1.solution()"
      ],
      "metadata": {
        "id": "FblT1XacazxL",
        "outputId": "b6f2b47d-7028-4754-bb70-932b18bd001d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.047619047619047616, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Exercise1\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Correct"
            ],
            "text/markdown": "<span style=\"color:#33cc33\">Correct</span>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJA3xvK39zUu"
      },
      "source": [
        "As we have already learned, there are many strategies to deal with these kinds of missing records or 'dirty' data. Here, we will use the brute-force version and simply remove it from the dataset. It is only one record and is completely missing. So, removing these records should be safe.\n",
        "\n",
        "First check that there is really only one record by displaying all null entries in the dataset. Run `congress_114[congress_114.isnull().any(axis=1)]`. `isnull()` checks for NaN entries and `any(axis=1)` applies this to all columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jWOSQsVj9zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP8hDzeq9zUu"
      },
      "source": [
        "That's only one record missing, and senator 'Markey' has not voted at all. This makes us confident that we can just delete them ...\n",
        "\n",
        "Remove the record. The easiest is to simply remove all the records with NaN values. Pandas has a function for that called `dropna()`. Apply it to `congress_114`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q0gMsv9B9zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2.check()\n",
        "#q2.hint()\n",
        "#q2.solution()"
      ],
      "metadata": {
        "id": "b6tKXEZha90E",
        "outputId": "b5453e92-e390-4ee5-8f9c-9845ee025a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 2, \"failureMessage\": \"Found 16 NaN values remaining in the DataFrame. Make sure you used dropna() and assigned the result back to congress_114.\", \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Exercise2\", \"learnToolsVersion\": \"0.3.5\", \"valueTowardsCompletion\": 0.0, \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Incorrect: Found 16 NaN values remaining in the DataFrame. Make sure you used dropna() and assigned the result back to congress_114."
            ],
            "text/markdown": "<span style=\"color:#cc3333\">Incorrect:</span> Found 16 NaN values remaining in the DataFrame. Make sure you used dropna() and assigned the result back to congress_114."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3RI2_UB9zUu"
      },
      "source": [
        "Check the last 5 elements again to make sure that the observation with NaN values (of the senator who missed votes) is really gone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vI-f4pfM9zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxAWO7b9zUu"
      },
      "source": [
        "Try to get a quick overview of the dataset and 'describe' it with `congress_114.describe()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sa42Yi139zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgcvcMl59zUu"
      },
      "source": [
        "This has not produced very useful outputs. Check the column types with `congress_114.dtypes` to understand why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2mPsN8EY9zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwJRsOhU9zUu"
      },
      "source": [
        "1s and 0s or object strings obviously do not produce sensible means or standard deviations. They are not numbers and have no real average. For our columns with 0 and 1, `describe()` will still calculate stats.\n",
        "\n",
        "While the mean and standard deviation are not directly interpretable for categorical-like votes, they are still interesting to investigate what the cross-Senate vote was like and how many diverging opinions there were. According to `congress_114.describe()`, bill 10, e.g., got all the votes, while for bill 9 the vote was very divided. How can you see this? Similarly, the quantiles at the bottom of the output can be interesting to understand the distribution. What do you see for bill 9?\n",
        "\n",
        "Finally, check how many democrats, republicans and independents are in `congress_114`. Type in `congress_114.iloc[:,2].value_counts()` to select column 3 ('party') with iloc and apply `value_counts()` (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7svFgX0O9zUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_to6lWiETGFj"
      },
      "source": [
        "Let's run a test  to see whether we got everything right up to now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_LqjBkPRTLtI",
        "outputId": "7f791747-f27d-4981-f86b-d04f40c7469c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your code...\n",
        "assert congress_114.iloc[:,2].value_counts().sum() > 55, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_mTURPN9zUu"
      },
      "source": [
        "We want to improve the content of the dataframe next and make the types fit better. To this end, we change the float types to integers.\n",
        "\n",
        "This is actually quite hard work in Pandas. You need to first select the right columns in the dataframe. You could just count the column numbers, as it is a very small dataframe. But you could also use the Pandas function `select_dtypes(['float'])` to select only the right columns, because the bills columns are the only float columns, as you have seen in the `dtypes` output. Run the following:\n",
        "\n",
        "```\n",
        "bill_cols = list(congress_114.select_dtypes(['float']).columns)\n",
        "bill_cols\n",
        "```\n",
        "We call them `bill_cols` as only the bills are `float` in `congress_114.dtypes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bbVt-0si9zUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ9KcX-o9zUv"
      },
      "source": [
        "With `bill_cols`, you can select all the bill columns. Make them all integer columns with `astype(int)`. Type in `congress_114[bill_cols] = congress_114[bill_cols].astype(int)`.\n",
        "\n",
        "BTW, you will have noticed that string types are objects in Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ap6Quem09zUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVwwy-j9zUv"
      },
      "source": [
        "Make sure that everything has come out as planned by checking `dtypes` next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SZFf9uv79zUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl-fUJdVIlT"
      },
      "source": [
        "Let's run a test that we got it all right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QjeI_nn8Tl2R",
        "outputId": "c00dec09-d33c-4fe3-a760-fb524726797c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your code...\n",
        "assert list(congress_114.dtypes.value_counts().items())[0][1]>=15, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbq2TkWoLVvE"
      },
      "source": [
        "Now, let's see whether Gemini can do the same. Run the cell below to create another dataframe `congress_114_gemini` with the 114_congress dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "egznLn9yK0_L"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "congress_114_gemini = pd.read_csv(\"114_congress.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJv30pKILhO8"
      },
      "source": [
        "Can you write a prompt that removes all rows with NaN values from `congress_114_gemini` and transforms float columns to integer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FPMqLmd2Kl2l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FExZLOFMLtm0"
      },
      "source": [
        "Run the cell below to see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "drq03AdYLLSt",
        "outputId": "6fe64d86-0e5a-4b84-a8db-ba454b5bb6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 58 entries, 0 to 57\n",
            "Data columns (total 19 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   index    58 non-null     int64  \n",
            " 1   name     58 non-null     object \n",
            " 2   party    58 non-null     object \n",
            " 3   state    57 non-null     object \n",
            " 4   bill-1   57 non-null     float64\n",
            " 5   bill-4   57 non-null     float64\n",
            " 6   bill-5   57 non-null     float64\n",
            " 7   bill-6   57 non-null     float64\n",
            " 8   bill-7   57 non-null     float64\n",
            " 9   bill-8   57 non-null     float64\n",
            " 10  bill-9   57 non-null     float64\n",
            " 11  bill-10  57 non-null     float64\n",
            " 12  bill-20  57 non-null     float64\n",
            " 13  bill-26  57 non-null     float64\n",
            " 14  bill-32  57 non-null     float64\n",
            " 15  bill-38  57 non-null     float64\n",
            " 16  bill-39  57 non-null     float64\n",
            " 17  bill-44  57 non-null     float64\n",
            " 18  bill-47  57 non-null     float64\n",
            "dtypes: float64(15), int64(1), object(3)\n",
            "memory usage: 8.7+ KB\n"
          ]
        }
      ],
      "source": [
        "#Keep cell\n",
        "congress_114_gemini.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBo0WfIa9zUv"
      },
      "source": [
        "### Clustering with k-means\n",
        "\n",
        "Let's practice clustering again with k-means. In order to find a good starting point for k, we can use our own knowledge about how the US senate is structured. We would like to investigate voting clusters, and we know that the US senate is dominated by 2 major parties. So, it seems like a good idea to start with two clusters (k = 2), as we can assume that there should be two major party-based voting clusters. Please, assign `k = 2`.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gQyNgakC9zUv"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "k = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWEXhDoJ9zUv"
      },
      "source": [
        "Next, we need to understand what we would like to cluster and choose the relevant features as inputs into the k-means algorithm. If you look back into your earlier explorations of the dataset, you can see that the first 4 columns do not contain voting behaviour. They have the name, state, etc. of the various senators. The voting behaviour can be found in columns 5 to 19. Use either the column indexes or `bill_cols` to create a new dataframe `congress_114_voting`, which only contains the voting behaviour.  Also, print out the first couple of rows of `congress_114_voting`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8CEg2nDC9zUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q3.hint()\n",
        "#q3.solution()"
      ],
      "metadata": {
        "id": "HwPrkbpzbKCg",
        "outputId": "b4dbbdc6-74db-416e-fa29-d39fa7a37c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 4, \"questionId\": \"3_Exercise3\", \"learnToolsVersion\": \"0.3.5\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hint: Create a subset of the DataFrame containing only the voting columns (bill_cols). Use .loc with column indexing: df.loc[:, column_list] selects all rows and specified columns. Then use .head() to display the first few rows of this subset."
            ],
            "text/markdown": "<span style=\"color:#3366cc\">Hint:</span> Create a subset of the DataFrame containing only the voting columns (bill_cols). Use .loc with column indexing: df.loc[:, column_list] selects all rows and specified columns. Then use .head() to display the first few rows of this subset."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqEGha-89zUv"
      },
      "source": [
        "Great, we are ready to cluster the votes. Check out the details of k-means in the `sklearn` documentation: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "\n",
        "Its main arguments are the dataset to cluster and the number of clusters. We can leave all the other inputs at their defaults.\n",
        "\n",
        "First `import KMeans` from `sklearn.cluster` by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AEozyztj9zUv"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JunLFS3o9zUv"
      },
      "source": [
        "Now, run `KMeans` and fit it with `n_clusters = k`. Check the documentation and you will find how to do it:\n",
        "```\n",
        "kmeans = KMeans(n_clusters = k)\n",
        "kmeans.fit(congress_114_voting)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7aEGxXJS9zUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9EPOzp49zUv"
      },
      "source": [
        "This should not have taken too long, as the dataset is very small.\n",
        "\n",
        "K-means is a fairly simply machine-learning algorithm, but still a standard example of an unsupervised machine-learning algorithm. Unsupervised machine learning means that you do not have to train the computer in advance about the kind of results you expect.\n",
        "\n",
        "Print out the number of iterations required to converge. Check the documentation and you will see that you need `kmeans.n_iter_`. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RvZEtFZn9zUv",
        "outputId": "21b309ff-4fac-42f0-eb8e-f87b082bc08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'kmeans' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-542652002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Keep cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'kmeans' is not defined"
          ]
        }
      ],
      "source": [
        "#Keep cell\n",
        "kmeans.n_iter_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RYURK50VdFV"
      },
      "source": [
        "Let's run a test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplQ9HzcVebl"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "assert kmeans.n_iter_ >= 2, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNtm5xgy9zUv"
      },
      "source": [
        "This is just one of the many pieces of information we can find in `kmeans`. The cluster assignments are stored as a one-dimensional NumPy array in `kmeans.labels_`. Here is a look at the first five predicted labels. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zpgt-rW9zUv"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "kmeans.labels_[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqZAAygd9zUv"
      },
      "source": [
        "This means the first 5 senators belong to these clusters with the count starting at 0.\n",
        "\n",
        "Ok, so now that we have run our machine-learning algorithms, what do we do with the results? A good first step for k-means and other clustering algorithms is to check out the size of the clusters. Who do you expect to belong to each cluster?\n",
        "\n",
        "Use `np.bincount` with `kmeans.labels_`, please, and run `np.bincount(kmeans.labels_)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnOBLWcZ9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaJq1NL29zUw"
      },
      "source": [
        "These numbers show that there is not really a clear division between Republicans and Democrats, as the clusters do not correspond to the numbers each party has in the senate.\n",
        "\n",
        "Create a new dataframe `congress_114_result`, which contains the first 4 columns of `congress_114` as well as the cluster assignments for each senator by k-means. Take a moment to reflect what we gain from such a new dataframe?\n",
        "\n",
        "Type in:\n",
        "```\n",
        "congress_114_result = congress_114.iloc[:, :4].copy()\n",
        "congress_114_result[\"cluster\"] = pd.Series(kmeans.labels_)\n",
        "```\n",
        "With `:4`, we select the first four columns - the non-bill columns. With `copy()`, we make a copy to keep the original dataframe so that we do not overwrite the old one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deyS8g1N9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweNsc4d9zUw"
      },
      "source": [
        "Because we like it tidy, we give the columns of `congress_114_result` new readable names. We can do this by assigning the columns directly to a list of names: `congress_114_result.columns = ['index','name','party', 'state', 'cluster']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6Rrv849zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJUx0CUO9zUw"
      },
      "source": [
        "Let's take a look at the composition of `congress_114_result` and print out the whole dataframe. What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj0MG9T59zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOg3JhLZ9zUw"
      },
      "source": [
        "Finally, let's take a look at the composition of our 2 clusters.\n",
        "\n",
        "In this case, we want to count how often a Democrat appears in cluster 0 or how often in cluster 1; similarly, how often is a Republican part of either cluster 1 or 2. Please note that there are also Independent senators.\n",
        "\n",
        "In order to compare party and cluster features, use `pd.crosstab`: https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html. With the crosstab function, we can count the frequency of the combination of two columns. Simply run: `pd.crosstab(congress_114_result['party'], congress_114_result['cluster'])`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I43y75O69zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeJQXWXt9zUw"
      },
      "source": [
        "Take a minute to interpret the results. Which party is more coherent in its voting behaviour? Can you identify the outliers by looking through the result dataframe?\n",
        "\n",
        "`k = 2` seems to have been a fairly good choice for kmeans, as there is a lot of overlap between parties and voting clusters.\n",
        "\n",
        "Let's try `k = 5` next to get more diversified results with 5 clusters.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qZry72S9zUw"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "k = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbwD9im19zUw"
      },
      "source": [
        "Fit k-means with the new `k`. Assign it to `kmeans_5`. Check out the earlier code for fitting k-means with `k=2`. Simply copy it and run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvE3NDlr9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi7DInZY9zUw"
      },
      "source": [
        "As we already have a `congress_114_result` dataframe, we just need to create a new column with the new cluster assignments. Create a new column `cluster_5` in congress_114_result using the result from `k_means5`. The columns should be filled by `np.array(kmeans_5.labels_)`. Note we use the array now to fill the column. Earlier we used `pd.Series`. There are many different ways of doing this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgRFAt539zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q4.check()\n",
        "#q4.hint()\n",
        "#q4.solution()"
      ],
      "metadata": {
        "id": "pbC2AWn9baKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFc2ObrfWX7z"
      },
      "source": [
        "Let's run a quick test that everything looks ok. We cannot really verify the detailed results as they are different for all of us, but the test at least checks whether we are on the right way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d8a3d9sWbkD"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "assert len(np.unique(np.unique(kmeans_5.labels_))) == 5, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neehqjbf9zUw"
      },
      "source": [
        "Let's run `congress_114_result` to check the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi9QNDfq9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqKBB4vA9zUw"
      },
      "source": [
        "Now, let's compare voting behaviour and parties again with `crosstab`. Can you change the earlier cell to compare  `cluster_5` and `party` in `congress_114_result`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0fgXiJu9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q5.hint()\n",
        "#q5.solution()"
      ],
      "metadata": {
        "id": "1kymcoP-b2TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVUNShCs9zUw"
      },
      "source": [
        "There is at least one strange outlier in the clusters with regard to voting behaviour of particular Republicans. Let's investigate those Republicans who appear not to vote with the rest of their party or other Democrats.\n",
        "\n",
        "The cluster number will change depending on the result of your k-means. So, anything between 0 and 4. You get this number from the table you have just printed out. Use it to filter `congress_114_result` and retrieve the names and states of the senators. So, for instance, in my current run the three Republicans were in cluster 4. So, I run:\n",
        "```\n",
        "clusters_of_interest = [4]\n",
        "congress_114_result.loc[congress_114_result['cluster_5'].isin(clusters_of_interest), ['name', 'state']]\n",
        "```\n",
        "`isin()` is a function to check whether something is part of a list. You might have to replace 4 in `clusters_of_interest = [4]` with your cluster number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp0DhDgt9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tSXFGzE9zUw"
      },
      "source": [
        "### Visualisation\n",
        "\n",
        "Finally, we want to also visualise the cluster assignment to present how senators are close to each other using a 2-dimensional coordinate system.\n",
        "\n",
        "Create a simple visualisation that maps the 5 clusters in 2 dimensions and colour the points that represent the senators according to their 5 k-means clusters.\n",
        "\n",
        "You need the principal component analysis (PCA) trick to map the 5 dimensions of all 5 clusters in `congress_114_voting` to 2 dimensions. It is explained here: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python.\n",
        "\n",
        "First, load PCA from sklearn with `from sklearn.decomposition import PCA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcWYkSQN9zUw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ZuYqxh9zUx"
      },
      "source": [
        "Next, apply a PCA with two components.\n",
        "\n",
        "Then, create a new dataframe `principal_df` with the results of this analysis and name the two columns `PC_1` and `PC2`.\n",
        "\n",
        "Finally, add the columns `name`, `party`, `cluster` and `cluster_5` to `principal_df`. Run:\n",
        "\n",
        "```\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "principal_components = pca.fit_transform(congress_114_voting)\n",
        "\n",
        "principal_df = pd.DataFrame(data = principal_components, columns = ['PC_1', 'PC_2'])\n",
        "\n",
        "principal_df['name'] = congress_114_result.name\n",
        "principal_df['party'] = congress_114_result.party\n",
        "principal_df['cluster'] = congress_114_result.cluster\n",
        "principal_df['cluster_5'] = congress_114_result.cluster_5\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dJS2HLM9zUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2pB37lV9zUx"
      },
      "source": [
        "Run the code below to visualise. Do you understand what it is doing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJJr0eRa9zUx"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "#based on https://honingds.com/blog/seaborn-scatterplot/\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(15,7))\n",
        "\n",
        "sns.scatterplot(data = principal_df, x='PC_1', y='PC_2', hue='cluster_5',\n",
        "                style='party', s=100, palette=['green','grey','red','blue','orange'], alpha=.40)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x']+.02, point['y'], str(point['val']))\n",
        "\n",
        "label_point(principal_df.PC_1, principal_df.PC_2, principal_df.name, plt.gca())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJVSuxWv9zUx"
      },
      "source": [
        "Take some time now and investigate which senators are the outliers are by researching them online. Wikipedia is enough. Can you understand why they are clusters by themselves?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haKJB3VjCOXF"
      },
      "source": [
        "Let's ask Gemini to run PCA for congress_114_voting with two dimensions and visualise the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4hQ9mbxBujx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwyn2PxQCW_N"
      },
      "source": [
        "Nice. For me, it is already very close to what we have created manually.\n",
        "\n",
        "Bonus: Go online or ask Gemini and research how to interpret PCA graphs with two dimensions? There are some common pitfalls to avoid ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWDs_MkP9zUx"
      },
      "source": [
        "### Determine the optimal number of parties in the Senate according to k-means\n",
        "\n",
        "So what would be the optimal number of parties in the senate? In terms of k-means, we need to determine the optimal `k`. There are quite a few methods to estimate `k`. Among the best known ones is the elbow method that is based on visualising trials with several k's: https://pythonprogramminglanguage.com/kmeans-elbow-method/.\n",
        "\n",
        "The elbow method is a useful graphical tool to estimate the optimal number of clusters `k` for a given task. Intuitively, we can say that, if `k` increases, the within-cluster 'Sum of Squared Errors' (SSE; also called 'distortion') will decrease. SSE is the sum of the squared differences between each observation and its group's mean: https://hlab.stanford.edu/brian/error_sum_of_squares.html.\n",
        "\n",
        "The idea behind the elbow method is to identify the value of `k` where the distortion begins to decrease most rapidly, which will become clearer if we plot the distortion for different values of `k`.\n",
        "\n",
        "To perform the elbow method, we need to run several k-means, increment `k` with each iteration until we reach `max_k = 10` - and record the SSE-score in a list called sse. Then, map the SSE for each iteration to find the point for curve bends, the elbow. This will be the best `k`.\n",
        "\n",
        "You could try this yourself, ask Gemini, or simply run the cells below. Don't forget to try and understand the code. You should recognize most of its components?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdEcDKDk9zUx"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "max_k = 10\n",
        "sse = []\n",
        "\n",
        "for k in range(1, max_k):\n",
        "    kmeans_ = KMeans(n_clusters = k)\n",
        "    kmeans_.fit(congress_114_voting)\n",
        "    sse.append(kmeans_.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSZTLWiS9zUx"
      },
      "source": [
        "Run the code below to visualise the elbow ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AIH0cra9zUx"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "elbow_df  = pd.DataFrame(list(zip(range(1, max_k), sse)),\n",
        "              columns = ['Number of Clusters', 'SSE'])\n",
        "\n",
        "\n",
        "sns.lineplot(x = 'Number of Clusters', y = 'SSE', data = elbow_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms3H1E_o9zUx"
      },
      "source": [
        "As you can see 2 is already the best answer ...\n",
        "\n",
        "You can now continue playing with different k values if you want.\n",
        "\n",
        "Before we move on, please finally consider https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means.\n",
        "\n",
        "The discussion describes that, while clustering (and other machine-learning algorithms) can produce very persuasive results, these do not come for free. They are no free lunch. The results always depend on the assumptions we add like the number of clusters in k-means but also how we describe the vote in the US Congress, how we measure somebody's influence, etc. This is the famous 'No Free Lunch Theorem' in machine learning: https://en.wikipedia.org/wiki/No_free_lunch_theorem.\n",
        "\n",
        "The theorem also applies to supervised machine learning, which we will practice again for the rest of the workshop.\n",
        "\n",
        "But first ask Gemini to create an elbow graph for k-means with `congress_114_voting`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1oFBnhNDVff"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUYzNmgG9zUx"
      },
      "source": [
        "## Supervised Machine Learning\n",
        "\n",
        "In the lecture, we explored how neural networks can be used to predict danceability. However, when working with structured, tabular data  such as spreadsheet-like or database-style datasets (https://en.wikipedia.org/wiki/Structured_data)  decision trees or their ensemble variants, like Random Forests (https://en.wikipedia.org/wiki/Decision_tree), are often more effective. These models have been around for a long time, yet they continue to deliver excellent performance on structured data. That's why now is a great time to learn about them. In fact, when dealing with structured data, Random Forests should often be your go-to model.\n",
        "\n",
        "As part of this exercise, you'll revisit the key steps of the machine-learning process. We begin with data preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJxDfxSu9zUx"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "Let's start with recreating the relevant dataframe `music_df`. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXltBvTP9zUx"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "DANCEABILITY_THRESHOLD = 0.6 # Let's say songs with danceability >= 0.6 are \"High Danceability\"\n",
        "\n",
        "music_raw_df = pd.read_csv('spotify-dataset.csv')\n",
        "music_raw_df['dance_label'] = np.where(music_raw_df['danceability'] >= DANCEABILITY_THRESHOLD, 1, 0)\n",
        "\n",
        "feature_cols = [\n",
        "    'valence', 'liveness', 'tempo', 'acousticness', 'energy', 'speechiness', 'instrumentalness', 'loudness', 'popularity'\n",
        "]\n",
        "feature_cols = feature_cols + ['dance_label']\n",
        "music_df = music_raw_df[feature_cols].copy()\n",
        "\n",
        "music_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onxc6FDq9zUx"
      },
      "source": [
        "Do you remember how to show the first rows of `music_df`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9mpTFZ39zUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFHtAIpU9zUx"
      },
      "source": [
        "Looking back at our neural-network example, the next step should be to apply min-max-scaling to prepare the machine learning. In the example, we first did it manually. sklearn does a much better job at this, with `MinMaxScaler`. Check out https://scikit-learn.org/stable/modules/preprocessing.html.\n",
        "\n",
        "We first import the `MinMaxScaler` class from sklearn's preprocessing module and create an instance of the scaler by calling `MinMaxScaler()`.\n",
        "\n",
        "Next, we make a copy of our data to create `music_normalized_df`.\n",
        "\n",
        "Finally, we fit the scaler to our data by calling `fit_transform()` on our training set `music_normalized_df`. This step applies the scaling transformation to each feature based on the previously calculated min and max values. Please, note that we should normally only fit the scaler with the training data to avoid leakage of data to test (https://en.wikipedia.org/wiki/Leakage_(machine_learning)), but we ignore this here for demonstration purposes.\n",
        "\n",
        "BTW, according to the documentation, https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html is another version of the scaling, which we have employed for the `teens` dataset without the help of sklearn.\n",
        "\n",
        "Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rABkG1-D9zUx"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "music_normalized_df = music_df.copy()\n",
        "music_normalized_df.iloc[:,:-1] = scaler.fit_transform(music_normalized_df.iloc[:,:-1])\n",
        "\n",
        "music_normalized_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-bPpE1QWC8c"
      },
      "source": [
        "\n",
        "Let's run a test that we normalized everthing correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWV6-Z1uYDuH"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "\n",
        "cond_ = np.isclose(music_normalized_df.iloc[0,0], 0.7186) and np.isclose(music_normalized_df.iloc[1,1], 0.101)\n",
        "assert cond_, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1yiOEtr9zUx"
      },
      "source": [
        "As a little exercise, try the same thing with https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html. Copy the code above but now use `MaxAbsScaler`.\n",
        "\n",
        "Create and print out `music_normalized_df2` as you have done for `music_normalized_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ltUmXdG9zUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q6.hint()\n",
        "#q6.solution()"
      ],
      "metadata": {
        "id": "GBYVspt7cBIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRG9FpFY9zUx"
      },
      "source": [
        "Can you see any differences?\n",
        "\n",
        "Actually, we do not need to normalize the data for decision trees and random forests. For these models, we just need to make sure that they preserve the order. You will later see why when we discuss the inner workings of these algorithms.\n",
        "\n",
        "We can thus continue working with `music_df`. Do you remember from the lecture what we want to do with this dataset? How do we beat music experts to understand danceability?\n",
        "\n",
        "Once we are happy with the data we have, we create a training and test dataset.\n",
        "\n",
        "Complete the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdUyH36F9zUx"
      },
      "outputs": [],
      "source": [
        "#Complete the cell\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#With random_state=42, we select the same test and training dataset for the random splits.\n",
        "train_set, test_set = train_test_split(music_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q7.check()\n",
        "#q7.hint()\n",
        "#q7.solution()"
      ],
      "metadata": {
        "id": "-7uWbWezcEAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Pnas6_9zUy"
      },
      "source": [
        "### The Workings of Decision Trees (optional)\n",
        "\n",
        "In this workshop, we learn more about supervised learning and in particular powerful methods for structured data. We apply what we have seen in the lecture, but we also move on to new and different data and explore decision trees in more detail.\n",
        "\n",
        "Feel free to skip this section at first and continue with the sections on sklearn's decision trees below. This part is mainly to understand the inner workings of these important algorithms, while in practice you will use the sklearn version directly.\n",
        "\n",
        "Decision trees are rule-based algorithms (https://en.wikipedia.org/wiki/Decision_tree). This means, they find rules through so-called splits across the various features of the dataset. They optimise which feature to select at which cut-off point to get the best possible split of the dataset according to the target. Let's see what this means in practice by going through the splits one step a time.\n",
        "\n",
        "As we do not know anything about the data yet, we simply decide that we want to split the data based on the first column/feature. It seems also like a good initial strategy to split according to the mean-value of the column. Can you determine in the next cell what the mean-value of `train_set['valence']` is? There is an `np` function for that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSiPjLmk9zUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "q8.hint()\n",
        "#q8.solution()"
      ],
      "metadata": {
        "id": "Tbv4aCbLcI3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbuhk5a9zUy"
      },
      "source": [
        "You should get ~0.47. Let's visualise the effects of a split on this column with the Seaborn library.\n",
        "\n",
        "Run the next cell to test this split on `test_set`.\n",
        "\n",
        "*Important*: You might get slightly different values for all the exercises because we work with random datasets. This is because your `test_set` might look differently. It is a good exercise to adjust the visualisation code accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3HR4uyj9zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "mean_valence = np.where(test_set['valence'] < 0.47, '< 0.47', '>= 0.47')\n",
        "\n",
        "fig,axs = plt.subplots(1,2, figsize=(11,5))\n",
        "sns.barplot(data=test_set, y=\"dance_label\", x=mean_valence, ax=axs[0]).set(title=\"Rate of valence\")\n",
        "sns.countplot(data=test_set, x=mean_valence, ax=axs[1]).set(title=\"Histogram of valence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z0MLSOv9zUy"
      },
      "source": [
        "The plot visually explores the relationship between a `valence` feature split at 0.47 and `dance_label` in the test dataset. The bar plot on the left shows the proportion of 'High Danceability' songs within each `valence` category, while the count plot on the right shows the distribution of songs across these `valence` categories. On the left, we see that the rate of 'High Danceability' differs fairly strongly between songs with low `valence` (< 0.47) and high `valence` (>= 0.47) in the test set. On the right, the number of songs in the test dataset are shown that fall into each `valence` category ('< 0.47' and '>= 0.47'). It shows that they are evenly distributed. This helps understand if `valence` (and the split at 0.47) is a good predictor for danceability and how many songs fall into each category. This is a step towards understanding potential split points for a decision tree. As the rate of valence is very uneven, we suspect that splitting `valence` at 0.47 is not a great split. Let's investigate further ...\n",
        "\n",
        "We now create a very simple model, which simply says that music of `valence` less than 0.47 is good (which equals to true in our case), while the ones above that value are bad. To do so, we evaluate with our `test_set` to see how accurate this approach turns out to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr4lfMl09zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "preds = test_set['valence'].values < 0.47\n",
        "preds = preds.astype(int)\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDmkIk3-9zUy"
      },
      "source": [
        "Let's check this with yet another metrics so that we learn a little more. Mean absolute error (MAE; https://en.wikipedia.org/wiki/Mean_absolute_error) measures the absolute error difference between the prediction and the actual value. In our case, the absolute error would be 0 if the predicted value corresponds to the actual one and 1 otherwise.\n",
        "\n",
        "Complete the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVgI0KvY9zUy"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "from sklearn.metrics import ___\n",
        "mean_absolute_error(test_set['dance_label'], ___)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q9.hint()\n",
        "#q9.solution()"
      ],
      "metadata": {
        "id": "A1_G8dDhcNYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMEwzurt9zUy"
      },
      "source": [
        "I am getting something like 0.66, which is not great. MAE of ~0.66 indicates many errors, as expected for a single, simple split.\n",
        "\n",
        "Alternatively, we could try another column. Finish the code below for the same split analysis using the column 'liveness'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1CB7teO9zUy"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "mean_liveness = ___(train_set['liveness'])\n",
        "preds = (test_set['liveness'].values < mean_liveness).astype(int)\n",
        "print('Preds: ', preds)\n",
        "print('Mean absolute error: ', mean_absolute_error(test_set['dance_label'], ___))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q10.hint()\n",
        "#q10.solution()"
      ],
      "metadata": {
        "id": "bxiPiMuscRMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUt1mr12ZhGF"
      },
      "source": [
        "This looks better. Why?\n",
        "\n",
        "Let's run a quick test that we are doing what we are supposed to be doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ9nyCrpZDA5"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "assert mean_absolute_error(test_set['dance_label'], preds) > 0.4, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lrXsKlt9zUy"
      },
      "source": [
        "Let's speed things up and try several columns and splits at the same time.\n",
        "\n",
        "We create a function `score_` to test how a split on a column affects the overall model we want to build. As we want to move slowly towards decision trees, we do not simply split on a column's average but on a measure called impurity (https://en.wikipedia.org/wiki/Decision_tree_learning). Impurity indicates how the two groups that a split creates are similar or dissimilar from each other. The smaller impurity is the better the split. We call these groups `left_group` and `right_group`.\n",
        "\n",
        "How do we measure group similarity? Similarity is often the big question in data analytics. In this case, we take the standard deviation of the target, which is `dance_label`. If you don't remember standard deviation from school, it is basically a measure of the amount of variation of values (https://en.wikipedia.org/wiki/Standard_deviation). Can you see where in the code below?\n",
        "\n",
        "Standard deviation or `std()` is thus a measure of the spread of two groups. If it is higher, the groups are more dissimilar. Then, we multiply by the number of data items in the groups, because a bigger group has more impact.\n",
        "\n",
        "We do this twice for both the left and the right split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxr1ONVk9zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "#The aim of score_ is to use a particular split point in a target variable to create similar groups according to the target column.\n",
        "\n",
        "def score_(column_values, target_values, split_point):\n",
        "    left_group_score = 0\n",
        "    right_group_score = 0\n",
        "\n",
        "    left_mask = column_values < split_point\n",
        "    left_count = left_mask.sum()\n",
        "    if left_count <= 1:\n",
        "        left_group_score = 0\n",
        "    else:\n",
        "        left_group_score = target_values[left_mask].std() * left_count\n",
        "\n",
        "    right_mask = column_values >= split_point\n",
        "    right_count = right_mask.sum()\n",
        "    if right_count <= 1:\n",
        "        right_group_score = 0\n",
        "    else:\n",
        "        right_group_score = target_values[right_mask].std() * right_count\n",
        "\n",
        "    return (left_group_score + right_group_score) / len(target_values) if len(target_values) > 0 else 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMjprc1D9zUy"
      },
      "source": [
        "Run `score_` with mean `valence` in the cell below. Observe how `score_` operates on `train_set` to find the best splits and then is tested with `test_set`. The splits are learned from training data and `test_set` is used only for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_9T27U69zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "mean_valence = np.mean(train_set['valence'])\n",
        "print(score_(test_set['valence'].values, test_set['dance_label'].values, mean_valence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEV9oEr29zUy"
      },
      "source": [
        "Please complete the code below for `acousticness`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-Vpsiar9zUy"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "mean_acousticness = np.mean(___['acousticness'])\n",
        "print(___(test_set['acousticness'].values, test_set['dance_label'].values, mean_acousticness))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q11.hint()\n",
        "#q11.solution()"
      ],
      "metadata": {
        "id": "wOf6BJJlcW8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Z68HPy9zUy"
      },
      "source": [
        "The scores are close, but mean valence seems to be a better split because the score is lower. Still not great but better.\n",
        "\n",
        "Let's play a bit and experience Jupyter notebook's interactive tools (https://jupyterbook.org/en/stable/interactive/interactive.html).\n",
        "\n",
        "Run the cell below and move the slider for different feature values.\n",
        "\n",
        "Please, note that we use the min_max scaled version of `music_df`, as the interaction widget is then easier to move. But the same behaviour could be observed with `train_set` and `test_set`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCZV6Zm99zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "from ipywidgets import interact\n",
        "def score__(nm, split):\n",
        "    col = music_normalized_df[nm]\n",
        "    return float(score_(col, music_normalized_df['dance_label'].values, split))\n",
        "\n",
        "\n",
        "interact(nm=list(music_normalized_df.columns[:-1]), split=0.5)(score__);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2cu4cN19zUy"
      },
      "source": [
        "As you can see, we can now use different values for the splits than the means only. Which ones increase the score and reduce impurity?\n",
        "\n",
        "Interesting, but also slow and not particularly automated. Let's use the computer to automatically find the best split point for a column.\n",
        "\n",
        "To find the best splits, we need to first find a number of unique split points. Let's try this for the first column `valence`. The code uses the `unique()` function from Pandas and then sorts the values with `sorted()`. Check in the documentation what these functions do.\n",
        "\n",
        "At the end, we print out only the first 15 `unq` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Aa-NCit9zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "col_ = 'valence'\n",
        "unq = train_set[col_].unique()\n",
        "[float(u) for u in sorted(unq)[:15]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAXjINcj9zUy"
      },
      "source": [
        "Let's score with these values. Run the cell below and check out what `argmin()` does in the documentation or ask Gemini. It looks for the smallest value and returns its index in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1_43W9b9zUy"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "scores = np.array([score_(train_set[col_], train_set['dance_label'], o) for o in unq if not np.isnan(o)])\n",
        "print(unq[scores.argmin()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFxyi5pu9zUy"
      },
      "source": [
        "For this column, 0.427 seems to be the best cut-off for the splits.\n",
        "\n",
        "A typical move in data analysis with Jupyter notebooks is to bring various cells together into one function to run them together. Let's define `min_column` using the previous two cells.\n",
        "\n",
        "Complete the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgSrytG79zUy"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "def min_column(df, col_):\n",
        "    df = df.sample(frac=0.1, random_state=4711) # For demonstration purposes, we take a sample of train_set to find the min column\n",
        "    unq = df[col_].dropna().unique()\n",
        "    s = np.array([score_(df[col_], df['dance_label'], o) for o in unq if not np.isnan(o)])\n",
        "    idx = s.argmin()\n",
        "    return (col_, float(unq[idx]), float(s[idx]))\n",
        "\n",
        "min_column(___, 'valence')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q12.hint()\n",
        "#q12.solution()"
      ],
      "metadata": {
        "id": "xkkwED5Xcb20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9grj1b79zUz"
      },
      "source": [
        "For all columns, we print out the name of the column, the split value and the impurity - ignoring the last column, which is `dance_label`, with the index -1.\n",
        "\n",
        "Complete the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiowj3IU9zUz"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "for w in list(train_set.columns[:-1]):\n",
        "    print(___(train_set, w))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q13.hint()\n",
        "#q13.solution()"
      ],
      "metadata": {
        "id": "TaJ6RHoHchSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJXDH6dS9zUz"
      },
      "source": [
        "According to my output, `valence <= 0.516` is the best split we can use. Why? Because it has the smallest value of impurity.\n",
        "\n",
        "We have (almost) re-invented the OneR machine-learning algorithms (https://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/ or https://christophm.github.io/interpretable-ml-book/rules.html). It's not very popular anymore but was very much used in the 1990s. It often provides a great baseline to compare more advanced models with. We can think of OneR and what we have done as a one-rule tree.\n",
        "\n",
        "How about we add another split after the first one with `valence`? We first pick the best split for `valence` and then find the best split for each of the two subgroups this split produces. To this end, we just have to repeat the previous step for each of the two groups based on a split of `valence <= 0.516`.\n",
        "\n",
        "Let's do this split first by running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhz7cvBP9zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "left_split_valence = train_set.loc[train_set['valence'] <= 0.516, :]\n",
        "right_split_valence = train_set.loc[train_set['valence'] > 0.516, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49-vLPpE9zUz"
      },
      "source": [
        "Let's repeat the previous step first for `left_split_valence`.\n",
        "To this end, we have to remove both the 'valence' and the 'music_label', as we do not want to split over valence again and do not need the target column.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHQu_pzi9zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "left_split_valence_columns = [c for c in left_split_valence.columns if c not in ['valence', 'dance_label']]\n",
        "for w in left_split_valence_columns:\n",
        "    print(min_column(left_split_valence, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXN43FwB9zUz"
      },
      "source": [
        "The best next split uses `tempo`.\n",
        "\n",
        "Now, repeat the same for `right_split_valence`.\n",
        "\n",
        "Complete the next cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete cell\n",
        "right_split_valence_columns = [c for c in ___ if c not in ['valence', 'dance_label']]\n",
        "for w in right_split_valence_columns:\n",
        "    print(min_column(right_split_valence, w))"
      ],
      "metadata": {
        "id": "EuQQMXfK5W3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q14.hint()\n",
        "#q14.solution()"
      ],
      "metadata": {
        "id": "ob7vGEQ3clnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_6UVwiBaZRl"
      },
      "source": [
        "What would be the next best split from the existing right split?\n",
        "\n",
        "The best split would be `tempo`. We could continue this for the four subsubgroups we have now and then repeat again and again the splits to build a decision tree.\n",
        "\n",
        "Let's run a test that we got everything right up to now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5A8OY85aCnO"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "assert min_column(right_split_valence, 'energy')[1]>0.8, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrjsCXjS9zUz"
      },
      "source": [
        "\n",
        "By increasingly adding these splits, we have created a decision tree. The model will start with the valence of music tracks and then add additional splits or rules until there are no more better splits.\n",
        "\n",
        "We do not want to do this now and rather use sklearn's `DecisionTreeClassifier` to do it for us.\n",
        "\n",
        "### Sklearn's Decision Trees\n",
        "\n",
        "For sklearn's decision trees, we need to create `X_train`, `y_train`, etc.\n",
        "\n",
        "Complete the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdWWFR4vh6Oq"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "X_train = train_set.loc[:, train_set.columns != 'dance_label'].values\n",
        "___ = train_set['dance_label'].values\n",
        "\n",
        "X_test = test_set.loc[:, ___ != 'dance_label'].values\n",
        "y_test = test_set['dance_label'].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q15.check()\n",
        "#q15.hint()\n",
        "#q15.solution()"
      ],
      "metadata": {
        "id": "kxp50UKhcqiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK1S9uV4wwII"
      },
      "source": [
        "Complete the code below to set up the model.\n",
        "\n",
        "Tip: `max_leaf_nodes=4` limits the size of the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_HpIhyS9zUz"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = ___(max_leaf_nodes=4, random_state=4711).fit(X_train, ___)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q16.check()\n",
        "#q16.hint()\n",
        "#q16.solution()"
      ],
      "metadata": {
        "id": "sqH73Yr4cuWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usnCCcZF9zUz"
      },
      "source": [
        "Draw the resulting tree for X_train. Before running this cell you might have to install graphviz on your system - if you are not on Colab: https://www.graphviz.org/download/.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH5izyHp9zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "import graphviz, re\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n",
        "    s = export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs)\n",
        "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n",
        "\n",
        "draw_tree(model, train_set.iloc[:,:-1], size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlMQDK2p9zUz"
      },
      "source": [
        "For me, these are the same splits that I have found earlier, although sklearn uses a different impurity measure than our simple one. It uses the 'gini' impurity measure.\n",
        "\n",
        "Each node contains information about the number of 'samples' or rows in the dataset that match the split. It shows as 'values' how many of these samples have as values 'bad' or 'good' danceability.\n",
        "\n",
        "But what is 'gini'? It is just another measure of impurity - very similar to our own home-made score. It is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRel007J9zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "def gini(dataset, cond):\n",
        "    act = dataset.loc[cond, 'dance_label']\n",
        "    return (1 - act.mean()**2 - (1-act).mean()**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEzlLcIb9zUz"
      },
      "source": [
        "This calculates the probability that, if you pick two rows from a group, you will get the same danceability. If the group is all the same, the probability is 1.0, and 0.0 if they're all different. So, in this case the higher the outcome the better the split.\n",
        "\n",
        "Let us try this for `valence`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhdqosb59zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "float(gini(train_set, train_set['valence'] <= 0.4)), float(gini(train_set, train_set['valence'] <= 0.51))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUPVx1I9zUz"
      },
      "source": [
        "`train_set['valence'] <= 0.51` is the better split with the higher gini value. This corresponds to the gini values in the above visualisation.\n",
        "\n",
        "Gini can be a bit complicated at first. So, as long as you remember that it is about impurities of the subgroups, you will be fine. Gini is often the default way of repeatedly splitting a decision tree as in our own example. So, you can just use it out of the box and don't need to change anything. But you need to always know a little bit about these measures - at least whether a higher or a lower value indicates a better split.\n",
        "\n",
        "Let us see how the decision tree compares with our own model.\n",
        "\n",
        "Complete the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McX1KZ079zUz"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "\n",
        "___ = model.predict(X_test)\n",
        "mean_absolute_error(y_test, preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q17.check()\n",
        "#q17.hint()\n",
        "#q17.solution()"
      ],
      "metadata": {
        "id": "Yf34vDQHcy3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4OJofRu9zUz"
      },
      "source": [
        "It is already better than the average liveness/valence models above, which had worse errors.\n",
        "\n",
        "We can do even better if make the decision tree deeper. You might have noticed that our original sklearn decision tree had a parameter of `max_leaf_nodes=4`, which says we only want 4 leaf nodes. With `min_samples_leaf=25`, we can increase the size of the tree. These are the minimum number of samples required to be at a leaf node. Check out https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html.\n",
        "\n",
        "Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS9vDJE89zUz"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "model_2 = DecisionTreeClassifier(min_samples_leaf=200, random_state=4711)\n",
        "model_2.fit(X_train, y_train)\n",
        "draw_tree(model_2, train_set.iloc[:,:-1], size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05CSgR2l9zUz"
      },
      "source": [
        "This looks impressively complex. Let us check out how the model does.\n",
        "\n",
        "Complete the next cell to run an evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iurtfunu9zUz"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "preds = model_2.predict(X_test)\n",
        "mean_absolute_error(y_test, ___)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q18.check()\n",
        "#q18.hint()\n",
        "#q18.solution()"
      ],
      "metadata": {
        "id": "8sy92RGDc4Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXK5h2Ny8PM"
      },
      "source": [
        "A bit better, but does this justify the complexity? This is a decision we have to make.\n",
        "\n",
        "Let's finally ask Gemini to create a training and test dataset for `music_df` with a train/test split of 0.2. Then, it should create a decision tree model and evaluate it with the mean absolute error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP780PFlr2zF"
      },
      "outputs": [],
      "source": [
        "# prompt: create training and test dataset for `music_df` with a test split of 0.2. Then, make a decision tree model and evaluate it with the mean absolute error\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(music_df.drop('dance_label', axis=1), music_df['dance_label'], test_size=0.2, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evh_JG9Z9zUz"
      },
      "source": [
        "Nice. This is what Gemini, GPT, etc. are really good at.\n",
        "\n",
        "Let's try something else and introduce the algorithm RandomForest, which is still used for state-of-the-art modelling of structured data.\n",
        "\n",
        "### Random Forest\n",
        "\n",
        "We do not really want to make the decision trees much more complex than what we have just seen.\n",
        "\n",
        "So, how can we go further from here? One idea was developed decades ago by Leo Breiman who suggested to create lots of trees and take the average of their predictions (Breiman, L. Random Forests. Machine Learning 45, 5-32 (2001). https://doi.org/10.1023/A:1010933404324).\n",
        "\n",
        "Taking the average like this, is also known as 'bagging' and can be done with many different models. For this so-called 'ensemble' of decision trees (https://en.wikipedia.org/wiki/Ensemble_learning), we have a particular name and call it RandomForest.\n",
        "\n",
        "The idea is to create random subsets from our data `music_df` and create uncorrelated models out of them. Using the average of these models' results, we should get a better estimation of the target value.\n",
        "\n",
        "The following will create a decision tree from a random subset of the training data and repeats this for 100 trees, determining the average mean absolute error.\n",
        "\n",
        "Run the two cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaPbataO9zU0"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "from numpy import random\n",
        "\n",
        "X_train = train_set.loc[:, train_set.columns != 'dance_label'].values\n",
        "y_train = train_set['dance_label'].values\n",
        "\n",
        "X_test = test_set.loc[:, test_set.columns != 'dance_label'].values\n",
        "y_test = test_set['dance_label'].values\n",
        "\n",
        "def get_tree_model(prop=0.75):\n",
        "    n = len(y_train)\n",
        "    #ids are random choice of numbers\n",
        "    ids = random.choice(n, int(n*prop))\n",
        "    return DecisionTreeClassifier(min_samples_leaf=5).fit(X_train[ids], y_train[ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYY61K3M9zU0"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "#Get 10 decision tree models\n",
        "tree_models = []\n",
        "for t in range(10): #create 10 trees\n",
        "    tree_models.append(get_tree_model())\n",
        "\n",
        "all_probs = []\n",
        "for tm in tree_models:\n",
        "    all_probs.append(tm.predict(X_test))\n",
        "\n",
        "#Calculate the average predictin for each row by taking the mean\n",
        "avg_probs = np.array(all_probs).mean(0)\n",
        "\n",
        "mean_absolute_error(y_test, avg_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrZUx0B9zU0"
      },
      "source": [
        "This is better than my decision tree, which we should expect. We are on a good track.\n",
        "\n",
        "What we have just created, is done better with sklearn's `RandomForestClassifier`, which follows the same principles but better. `RandomForestClassifier` offers some extra tricks such as picking a random subset of columns for each split.\n",
        "\n",
        "Let us create everything in one go with sklearn. Complete the following code by using the sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-4Fewjx9zU0"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = ___(10, min_samples_leaf=5, random_state=4711)\n",
        "rf.fit(___, y_train);\n",
        "mean_absolute_error(y_test, rf.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q19.check()\n",
        "#q19.hint()\n",
        "#q19.solution()"
      ],
      "metadata": {
        "id": "Rnln4zgoc87g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbiIEdA0cIzw"
      },
      "source": [
        "This is significantly better and better than my neural net from the lecture. As you can see, it is hard to beat an algorithm from 2001 for structured data analysis.\n",
        "\n",
        "One more test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwD_qXTLbG0e"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your code...\n",
        "cond_ = pd.Series(rf.feature_importances_, index=train_set.columns[:-1]).sort_values()[-1:].index.values[0]=='valence'\n",
        "assert cond_, f'Test failed'\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_zwK0GR9zU0"
      },
      "source": [
        "Another nice aspect of random forests is they can tell us which inputs are most important for the predictions, using `feature_importances_`.\n",
        "\n",
        "Let's try to re-implement the example https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html. We ignore the standard derivation discussion.\n",
        "\n",
        "Complete the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_LZZR289zU0"
      },
      "outputs": [],
      "source": [
        "#Complete cell\n",
        "import pandas as pd\n",
        "\n",
        "forest_importances = pd.Series(rf.feature_importances_, index=train_set.columns[:-1])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.sort_values(ascending=___).plot.bar(ax=ax) #Sorting as well ...\n",
        "ax.set_title(\"Feature importances\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q20.hint()\n",
        "#q20.solution()"
      ],
      "metadata": {
        "id": "Ap5NWfmPdBxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valence, tempo and acousticness are the three most important features.\n",
        "\n",
        "Valence is clearly dominating though. Let's have a quick look what a maximum combination of `valence` and `danceability` is. The next video is the song in the dataset with the maximum product of `valence` and `danceability`."
      ],
      "metadata": {
        "id": "73-yfjVr5OTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep Cell\n",
        "#Multiplying valence an danceability and then selecting the max of product  gives us ... Oh well ...\n",
        "\n",
        "combined_example = \"-RSIIu0WWcM\"\n",
        "display_youtube(combined_example, video_title=\"Example of most danceable music with highest valence\")"
      ],
      "metadata": {
        "id": "T1Yf2fry42TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqyANUkF9zU0"
      },
      "source": [
        "Obviously the kids are dominating the dance charts ...\n",
        "\n",
        "And of course, Gemini can also do everything we have done. Ask Gemini to \"Create training and test dataset for `music_df` with a test split of 0.2. Then, make a random forest model and evaluate it with the mean absolute error. Finally, visualise the feature importance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbQfBDbCucHV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZMJZ6Qb9zU0"
      },
      "source": [
        "Impressive! Why not practice more now at https://www.w3schools.com/python/python_ml_decision_tree.asp?\n",
        "\n",
        "By understanding the principles of decision trees and random forests, you have a great option to analyse most structured data. I am proud of you.\n",
        "\n",
        "That's it for machine learning 101! That was not too hard, was it?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save to html (uncomment to execute)"
      ],
      "metadata": {
        "id": "uUyWHuGTLRMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_this_notebook = 'INSERT PATH TO THIS NOTEBOOK HERE'\n",
        "# !jupyter nbconvert \"{path_to_this_notebook}\" --to html"
      ],
      "metadata": {
        "id": "ocYpup5fLQQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "message,tags,strip.white,-all",
      "main_language": "R",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}